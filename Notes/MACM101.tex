\documentclass[12pt]{article}
\usepackage{amsmath}
%\usepackage{systeme}
\usepackage{amsthm}
\usepackage{array}
\usepackage{algpascal}
\usepackage{amssymb}
\usepackage{circuitikz}
\usepackage{adjustbox}
\usepackage{cancel}
\usepackage[shortlabels]{enumitem}
\newcolumntype{L}[1]{>{\raggedright\let\newline\\\arraybackslash\hspace{0pt}}m{#1}}
\newcolumntype{C}[1]{>{\centering\let\newline\\\arraybackslash\hspace{0pt}}m{#1}}
\newcolumntype{R}[1]{>{\raggedleft\let\newline\\\arraybackslash\hspace{0pt}}m{#1}}
\newtheorem{definition}{Definition} [section]
\newtheorem{proposition}{Proposition} [subsection]
\newtheorem{theorem}{Theorem} [section]
\newtheorem{corollary}{Corollary} [theorem]
\newtheorem{lemma}{Lemma} [section]

\title{MACM 101}
\author{Dr. C. Kay Wiese}
\date{}

\begin{document}

\maketitle

\section{Counting}

\subsection{The Rules of Sums and Products}

Be careful of initial conditions (duplicates and assumptions)
\\
\underline{Rules of Sums}

If task A can be performed in $m$ ways, while task B can be performed in $n$ ways and A and B cannot be done simultaneously, then performing either task can be done in any one of $m+n$ ways
\\
\underline{Rules of Products}

A procedure P can be broken down into A and B stage. If A has $m$ outcomes and B has $n$ outcomes, P can be carried out in $m * n$ ways.
\subsection{Permutations}
\begin{itemize}
\item Distinct Objects
\item Linear arrangement objects, i.e.\ the \emph{order} of objects is important
\end{itemize}
\begin{definition}Factorials\end{definition} For integer $n \geq 0$,
\[
n!=\begin{cases} 1 & n=0 \\ n*(n-1)!&n\geq1\end{cases}
\]
\begin{definition}\end{definition} \noindent If there are $n$ distinct objects and $ 1 \leq r \leq n $, then, by rule of product, the number of permutations of size $r$ for the $n$ objects is
\[
P(n, r) = \frac{n!}{(n-r)!}
\]
\subsection{Combinations}
\begin{definition}\end{definition} \noindent If there are $n$ distinct objects and $ 1 \leq r \leq n $, then the number of combinations of size $r$ for the $n$ objects is
\[
\binom{n}{k}=C(n, r) = \frac{n!}{(n-r)!r!}
\]
\\
You can use a combinatorial argument in proofs.
\begin{proposition} For positive integers $n$ and $k$ with $n=2k, \frac{n!}{2!^k}$ is an integer. \end{proposition}
\emph{Proof.}  Consider the $n$ symbols: $x_1, x_1, x_2, x_2, \cdots, x_k, x_k$.
The number of arrangements of all these $n = 2k$ symbols is an integer that equals
\[
\frac{n!}{\underbrace{2! 2! \cdots 2!}_{\text{k factors of 2!}}} = \frac{n!}{2!^k}
\]
\begin{definition}Sigma notation\end{definition}
\[
a_m + a_{m+1} + a_{m+2} + \cdots + a_{m+n} = \sum_{i = m}^{m+n} a_i
\]
\begin{definition}Weight\end{definition}
Weight of a string $X = x_1 x_2 \dots x_n$ is defiined as wt($X$) = $\sum_{i=1}^n x_i$
\begin{theorem}Binomial Theorem\end{theorem}
\[
(x+y)^n = \sum_{i=0}^{n} \binom{n}{i} x^i y^{n-i}
\]
\begin{corollary}\end{corollary} Set $x=y=1$, then it follows that 
\[
\sum_{i=0}^n \binom{n}{i} = 2^n
\]
\begin{corollary}\end{corollary} Similary, set $x = -1$ and $y = 1$, then it follows that 
\[
\sum_{i=0}^n -1^i \binom{n}{i} = 0
\]
\begin{theorem}Multinomial Theorem\end{theorem} With integers $n, t > 0$, the coefficient of $ x_1^{n_1}x_2^{n_2}\cdots x_t^{n_t}$ in the expansion of $(x_1 + x_2 + \cdots + x_t)^n$ is
\[
\frac{n!}{n_1!n_2! \cdots n_t!} = \binom{n}{n_1, n_2, \cdots n_t}
\]
where each $n_i$ is an integer with $0 \leq n_i \leq n$, for all $1 \leq i \leq t$, and $n_1 + n_2 + \cdots + n_t = n$.\\

\emph{Proof.} Choose $x_1$ from $n_1$ out of $n$ factors, then choose $x_2$ from $n_2$ out of $n - n_1$ factors, and so on. This gives 
\begin{eqnarray*}
& \displaystyle \binom{n}{n_1} \binom{n-n_1}{n_2} \binom{n-n_1-n_2}{n_3} \cdots \binom{n-n_1-n_2- \cdots - n_{t-1}}{n_t} \\
= & \displaystyle \frac{n!}{n_1!(n-n_1)!} \frac{(n-n_1)!}{n_2!(n-n_1-n_2)!} \cdots \frac{(n-n_1-n_2- \cdots - n_{t-1})!}{n_t!(n-n_1-n_2- \cdots - n_{t-1}-n_t)!} \\
= & \displaystyle \frac{n!}{n_1!n_2! \cdots n_t!}
\end{eqnarray*}
\subsection{Combinations with Repetition}
The number of ways to select $r$ of $n$ distinct objects with repetitions is
\[
\binom{n+r-1}{r}
\]
It is equivalent to the number of ways to separate $r$ identical stones with $n-1$ identical sticks where there are $n$ slots to represent how many times the $n$th object was chosen with the number of stones. 

Same logic can be used for counting how many ways $r$ objects can be distributed to $n$ containers, or how many ways $n$ nonnegative integers can add up to $r$ (order matters).

You can also count the number of execution of such codes:
\begin{algorithmic}
\State $counter := 0;$
\For{i=1}{n}
\For{j=1}{i}
\For{k=1}{j}
\State $counter := counter + 1;$
\end{algorithmic}
It is equivalent to counting how many triples of $(i, j, k)$ satisfy $1 \leq k \leq j \leq i \leq n$, which is choosing 3 numbers from $n$ numbers with repetitions. $counter$ would be $\displaystyle \binom{n+3-1}{3}$.
\pagebreak
\section{Fundamentals of Logic}
\subsection{Basic Connectives and Truth Tables}
\begin{definition}{\emph{Declarative sentences that are either true or false are called} statements\emph{(or} propositions\emph{), and we use lowercase letters of the alphabet to represent such statements.}}
\end{definition}
\emph{Primitive} statements cannot be broken down into anything simpler, and new statements can be obtained from existing ones in two ways.
\begin{enumerate}
\item Transform a given statement $p$ to $\neg p$ (Not $p$).\\
\item Combine two or more statements into a \emph{compound} statement, using one of the \emph{logical connectives}.
\begin{enumerate}
\item Conjunction: $p \wedge q$ ($p$ and $q$)
\item Disjunction: 
\begin{enumerate}
\item$p \vee q$ ($p$ or $q$)
\item $p \veebar q$
\end{enumerate}
\item Implication: $p \rightarrow q$ ($p$ implies $q$)
\item Biconditional: $p \leftrightarrow q$ ($p$ if and only if $q$)
\end{enumerate}
\end{enumerate}
Here is the truth table.\footnote{Sometimes, 0 and 1 are used for F and T instead, similar to bit-logic.}
\begin{center}
\begin{tabular}{|c|c|c|c|c|c|c|}
\hline
$p$ & $q$ & $p \wedge q$ & $p \vee q$ & $p \veebar q$ & $p \rightarrow q$ & $p \leftrightarrow q$ \\
\hline
T & T & T & T & F & T & T \\
\hline
T & F & F & T & T & F & F \\
\hline
F & T & F & T & T & T & F \\
\hline
F & F & F & F & F & T & T \\
\hline
\end{tabular}
\end{center}
\begin{definition}
\emph{A compound statement is called a} tautology \emph{if it is always true. If it is always false, it is called a }contradiction. 
\end{definition}
We use the symbol $T_0$ to denote any tautology and the symbol $F_0$ to denote any contradiction.
\subsection{Logical Equivalence: The Laws of Logic}
\begin{definition}\emph{Two statements $s_1, s_2$ are said to be }logically equivalent \emph{when $s_1 \leftrightarrow s_2$, and we write $s_1 \Leftrightarrow s_2$}.
\end{definition}
\noindent If 2 statements are not logically equivalent, we write $s_1 \nLeftrightarrow s_2 \quad (\neg(s_1 \Leftrightarrow s_2))$.
\begin{center}
\textbf{The Laws of Logic}
\end{center}
\begin{tabular}  {c l l}
1) & $\neg \neg p \Leftrightarrow p$ & Law of Double Negation\\
2) & $\neg (p \wedge q) \Leftrightarrow \neg p \vee \neg q$ & DeMorgan's Laws\\
& $\neg (p \vee q) \Leftrightarrow \neg p \wedge \neg q$\\
3) & $p \wedge q \Leftrightarrow q \wedge p$ & Commutative Laws\\
& $p \vee q \Leftrightarrow q \vee p$\\
4) & $(p \wedge q) \wedge r \Leftrightarrow p \wedge (q \wedge r)$ & Associative Laws\\
& $(p \vee q) \vee r \Leftrightarrow p \vee (q \vee r)$\\
5) & $p \wedge (q \vee r) \Leftrightarrow (p \wedge q) \vee (p \wedge r)$ & Distributive Laws\\
& $p \vee (q \wedge r) \Leftrightarrow (p \vee q) \wedge (p \vee r)$\\
6) & $p \vee p \Leftrightarrow p$ & Idempotent Laws\\
& $p \wedge p \Leftrightarrow p$\\
7) & $p \vee F_0 \Leftrightarrow p$ & Identity Laws\\
& $p \wedge T_0 \Leftrightarrow p$\\
8) & $p \vee \neg p \Leftrightarrow T_0$ & Inverse Laws\\
& $p \wedge \neg p \Leftrightarrow F_0$\\
9) & $p \wedge F_0 \Leftrightarrow F_0$ & Domination Laws\\
& $p \vee T_0 \Leftrightarrow T_0$\\
10) & $p \vee (p \wedge q) \Leftrightarrow p$ & Absorption Laws\\
& $p \wedge (p \vee q) \Leftrightarrow p$
\end{tabular}
\\ \\
Following statements are also equivalent.
\begin {enumerate}
\item $p \rightarrow q \Leftrightarrow \neg p \vee q$
\item $p \leftrightarrow q \Leftrightarrow (p \rightarrow q) \wedge (q \rightarrow p) \Leftrightarrow (\neg p \vee q) \wedge (\neg q \vee p)$
\item $p \veebar q \Leftrightarrow (p \vee q) \wedge \neg (p \wedge q)$
\end{enumerate}
Using the above logival equivalences, we can eliminate those three connectives$(\rightarrow, \leftrightarrow, \veebar)$ from any logical compound statements.
\pagebreak
\begin{definition}
\emph{Let $s$ be a statement containing only $\wedge$ and $\vee$ as logical connectives. The dual of $s$, denoted $s^d$, is derived from $s$ by replacing each $\wedge$ with $\vee$, $\vee$ with $\wedge$, $T_0$ with $F_0$, and $F_0$ with $T_0$.}
\end{definition}
\noindent If $p$ is a primitive statement, then $p_d$ is the same as p.
\[
p = p^d
\]
\begin{theorem}
The Principle of Duality. \emph{Let $s$ and $t$ be statements containing no logical connectives other than $\wedge$ and $\vee$. If $s \Leftrightarrow t$, then $s^d \Leftrightarrow t^d$}.
\end{theorem}
\subsubsection{The Subtitution Rules}
\begin{enumerate}
\item Suppose, compound statement $P$ is a tautology. If $p$ is a primitive statement that appears in $P$ and we replace all occurrences of $p$ by an arbitrary statement $q$, then the resulting compound statement $P_1$ is also a tautology.
\item $P$ is a compound statement, $p$ is an arbitrary statement that appears in $P$, let $q$ be a statement such that $p \Leftrightarrow q$. Now replace 1 or more occurrences of $p$ by $q$, this yields $P_1$. Now, $P_1 \Leftrightarrow P$.
\end{enumerate}

\subsubsection{Relatives of the Implication $p \rightarrow q$}
\begin{tabular}{l c}
Inverse: & $\neg p \rightarrow \neg q$\\
Converse: & $q \rightarrow p$\\
Contrapositive: & $\neg q \rightarrow \neg p$\\
\end{tabular}
\begin{center}
\begin{tabular}{c l l}
1) & $p \rightarrow q \Leftrightarrow \neg q \rightarrow \neg p$ & Implication $\Leftrightarrow$ Contrapositive\\
2) & $q \rightarrow p \Leftrightarrow \neg p \rightarrow \neg q$ & Converse $\Leftrightarrow$ Inverse\\
3) & $p \rightarrow q \nLeftrightarrow q \rightarrow p$ & Implication $\nLeftrightarrow$ Converse\\
4) & $\neg p \rightarrow \neg q \nLeftrightarrow \neg q \rightarrow \neg p$ & Contrapositive $\nLeftrightarrow$ Inverse
\end{tabular}
\end{center}

\noindent An implication is logically equivalent to its contrapositive, but not to its inverse or converse. At the same time, its inverse and converse are logically equivalent. \\\\
(\emph{Note:} The negation of an it-then statement (in words) does not begin with the word if. i.e. it is not another implication.)
\subsubsection{Applications: Simplifying Switching Networks}
A switching network is made up of wires and switches connecting two Terminals $T_1$ and $T_2$. In such a network, each switch is either open (0) so current doesn't flow, or closed (1) so current does flow through it. Switches in \emph{parallel} are represented by $\vee$, and switches in \emph{series} are represented by $\wedge$.
\begin{center}
\begin{circuitikz} \draw
(0,1) node[left] {$T_1$}

(0,1) to (1,1)
(1,2) to[normal closed switch, l=$p$] (3,2)
(1,2) to (1,1)
(3,2) to (3,1)
(1,1) to[normal closed switch, l=$q$] (3,1)
(1,0) to (1,1)
(3,0) to (3,1)
(1,0) to[normal closed switch, l=$r$] (3,0)
(3,1) to (4,1)

(3,1) to (3,1)
(4,2) to[normal closed switch, l=$p$] (6,2)
(4,2) to (4,1)
(6,2) to (6,1)
(4,1) to[normal closed switch, l=$t$] (6,1)
(4,0) to (4,1)
(6,0) to (6,1)
(4,0) to[normal closed switch, l=$\neg q$] (6,0)
(6,1) to (7,1)

(6,1) to (6,1)
(7,2) to[normal closed switch, l=$p$] (9,2)
(7,2) to (7,1)
(9,2) to (9,1)
(7,1) to[normal closed switch, l=$\neg t$] (9,1)
(7,0) to (7,1)
(9,0) to (9,1)
(7,0) to[normal closed switch, l=$r$] (9,0)
(9,1) to (10,1)

(10,1) node[right] {$T_2$}
;
\end{circuitikz}
\end{center}
Such network can be represented by the statement $(p \vee q \vee r) \wedge (p \vee t \vee \neg q) \wedge (p \vee \neg t \vee r)$. This can be simplified using the laws of logic.
\begin{center}
\begin{tabular} {l l l}
& $(p \vee q \vee r) \wedge (p \vee t \vee \neg q) \wedge (p \vee \neg t \vee r)$ & \textbf{Reasons}\\
$\Leftrightarrow$ & $p \vee [(q \vee r) \wedge (t \vee \neg q) \wedge (\neg t \vee r)]$ & Distributive Law of $\wedge$ over $\vee$\\
$\Leftrightarrow$ & $p \vee [(q \vee r) \wedge (\neg t \vee r) \wedge (t \vee \neg q)]$ & Commutative Law of $\wedge$\\
$\Leftrightarrow$ & $p \vee [((q \wedge \neg t) \vee r) \wedge (t \vee \neg q)]$ & Distributive Law of $\wedge$ over $\vee$\\
$\Leftrightarrow$ & $p \vee [((q \wedge \neg t) \vee r) \wedge (\neg \neg t \vee \neg q)]$ & Law of Double Negation\\
$\Leftrightarrow$ & $p \vee [((q \wedge \neg t) \vee r) \wedge \neg (\neg t \wedge q)]$ & DeMorgan's Law\\
$\Leftrightarrow$ & $p \vee [\neg (\neg t \wedge q) \wedge ((\neg t \wedge q) \vee r)]$ & Commutative Law of $\wedge$\\
$\Leftrightarrow$ & $p \vee [(\neg (\neg t \wedge q) \wedge (\neg t \wedge q)) \vee (\neg (\neg t \wedge q) \wedge r)]$ & Distributive Law of $\wedge$ over $\vee$\\
$\Leftrightarrow$ & $p \vee [F_0 \vee (\neg (\neg t \wedge q) \wedge r)]$ & $s \wedge \neg s \Leftrightarrow F_0$ for any statement $s$\\
$\Leftrightarrow$ & $p \vee [\neg (\neg t \wedge q) \wedge r]$ & $F_0$ is the Identity for $\vee$\\
$\Leftrightarrow$ & $p \vee [r \wedge \neg (\neg t \wedge q)]$ & Commutative Law of $\wedge$\\
$\Leftrightarrow$ & $p \vee [r \wedge (t \vee \neg q)]$ & DeMorgan's Law and \\
&&the Law of Double Negation
\end{tabular}
\end{center}
\subsection{Logical Implication: Rules of Inference}
We show an argument with premises $(p_1, p_2, \cdots, p_n)$ and conclusion $q$ is valid by showing the following implication is a tautology:
\[
(p_1 \wedge p_2 \wedge \cdots \wedge p_n) \rightarrow q
\]
To show the following implication is a tautology, we need to show that $q$ is true if all the premises are true.

You can incorporate this into an automatic "inference engine", the basic component of an AI expert system. These systems combine basic facts to develop more facts.
\begin{definition}
\emph{If $p, q$ are arbitrary statements such that $p \rightarrow q$ is a tautology, then we say that $p$} logically implies \emph{$q$ and we write $p \Rightarrow q$ to denote this situation.}
\end{definition}
The notation $p \nRightarrow q$ is used to indicate that $p \rightarrow q$ is \emph{not} a tautology --- so the given implication ($p \rightarrow q$) is not a logical implication.
If $p \Leftrightarrow q$, then $p \leftrightarrow q$ is a tautology. This means $p \rightarrow q$ and $q \rightarrow p$ are tautologies, too, thus $p \Rightarrow q$ and $q \Rightarrow p$. The converse is also true.

When establishing the validity of an argument, the rules of inferences will enable us to consider only the cases wherein all the premises are true. They are fundamental in the development of a step-by-step validation of how the conclusion $q$ logically follows from the premises $p_1, p_2, \cdots , p_n$ in an implication of the form \[(p_1 \wedge p_2 \wedge \cdots \wedge p_n) \rightarrow q.\] This development will establish the validity of the given arguement, for it will show how the truth of the conclusion can be deduced from the truth of the premises.
\begin{center}
\addtolength{\leftskip} {-2cm} % increase (absolute) value if needed
\addtolength{\rightskip}{-2cm}
%\textbf{\large Rules of Inference}
\begin{tabular} {|l |l| l| C{4cm}|}
\hline
& \textbf{Rule of Inference} & \textbf{Related Logical Implication} & \textbf{Name of Rule}\\
\hline
1) & \begin{tabular} {c l}& $p$\\& $p \rightarrow q$\\\hline$\therefore$ & $q$\end{tabular} & $[p \wedge (p \rightarrow q)] \rightarrow q$ & Rule of Detachment / Modus Ponens\\
\hline
2) & \begin{tabular} {c l}& $p \rightarrow q$\\& $q\rightarrow r$\\\hline$\therefore$ & $p \rightarrow r$\end{tabular} & $[(p \rightarrow q) \vee (q \rightarrow r)] \rightarrow (p \rightarrow r)$ & Law of the Syllogism\\
\hline
3) & \begin{tabular} {c l}& $p \rightarrow q$\\& $\neg q$\\\hline$\therefore$ & $\neg p$\end{tabular} & $[(p \rightarrow q) \wedge \neg q)] \rightarrow \neg p$ &Modus Tollens \\
\hline
4) & \begin{tabular} {c l}& $p$\\& $q$\\\hline$\therefore$ & $p \wedge q$\end{tabular} & & Rule of Conjunction\\
\hline
5) & \begin{tabular} {c l}
& $p \vee q$\\
& $\neg p$\\
\hline
$\therefore$ & $q$
\end{tabular} & $[(p \vee q) \wedge \neg p] \Rightarrow q$ & Rule of Disjunctive Syllogism\\
\hline
6) & \begin{tabular} {c l}
& $\neg p \rightarrow F_0$\\
\hline
$\therefore$ & $p$
\end{tabular}
 & $(\neg p \rightarrow F_0) \rightarrow p$ & Rule of Contradiction\\
\hline
7) & \begin{tabular} {c l}
& $p \wedge q$\\
\hline
$\therefore$ & $p$
\end{tabular}
 & $(p \wedge q) \rightarrow p$ & Rule of Conjunctive Simplification\\
\hline
8) & \begin{tabular} {c l}
& $p$\\
\hline
$\therefore$ & $p \vee q$
\end{tabular}
 & $p \rightarrow p \vee q$ & Rule of Disjunctive Amplification\\
\hline
9) & \begin{tabular} {c l}
& $p \wedge q$\\
& $p \rightarrow (q \rightarrow r)$\\
\hline
$\therefore$ & $r$
\end{tabular}
 & $[(p \wedge q) \wedge [p \rightarrow (q \rightarrow r)]] \rightarrow r$ & Rule of Conditional Proof\\
\hline
10) & \begin{tabular} {c l}
& $p \rightarrow r$\\
& $q \rightarrow r$\\
\hline
$\therefore$ & $(p \vee q) \rightarrow r$
\end{tabular}
 & $[(p \rightarrow r) \wedge (q \rightarrow r)] \rightarrow [(p \vee q) \rightarrow r]$ & Rule for Proof Cases\\
\hline
11) & \begin{tabular} {c l}
& $p \rightarrow q$\\
& $r \rightarrow s$\\
& $p \vee r$\\
\hline
$\therefore$ & $q \vee s$
\end{tabular}
 & $[(p \rightarrow q) \wedge (r \rightarrow s) \wedge (p \vee r)] \rightarrow (q \vee s)$ & Rule of the Constructive Dilemma\\
\hline
12) & \begin{tabular} {c l}
& $p \rightarrow q$\\
& $r \rightarrow s$\\
& $\neg q \vee \neg s$\\
\hline
$\therefore$ & $\neg p \vee \neg r$
\end{tabular}
 & $[(p \rightarrow q) \wedge (r \rightarrow s) \wedge (\neg q \vee \neg s)] \rightarrow (\neg p \vee \neg r)$ & Rule of Destructive Dilemma\\
\hline
\end{tabular}
\end{center}
\pagebreak
The Rule of Contradiction is the basis of a method of \emph{Proof by Contradiction}, or \emph{Reductio ad Absurdum}.\
In a Proof of Contradiction, we establish the validity of the argument
\[
(p_1 \wedge p_2 \wedge \cdots \wedge p_n) \rightarrow q
\]
by establishing the validity of the logically equivalent argument
\[
(p_1 \wedge p_2 \wedge \cdots \wedge p_n \wedge q) \rightarrow F_0.
\] 
We first assume that what we are trying to prove is actually false, then produce a contradiction of the form $s \wedge \neg s$, for some statement $s$. This contradiction concludes that the statement that was assumed to be false is in fact true, and this validates the argument (or completes the proof).
\subsection{Predicate Logic and Quantifiers}
\begin{definition} \emph{A declarative sentence is an }open statement \emph{if}\end{definition}
\begin{enumerate}
\item It contains one or more variables, and
\item It is not a statement, but
\item It becomes a statement when the variables in it are replaced by certain allowable choices.
\end{enumerate}
All of these are open statements: \begin{center}The number $x-5$ is an even integer.\\ $x-5=7$\\$3x+y>7$\end{center}
The allowable choices of variable constitute what is called the \emph{universe}\footnote{This is an example of a \emph{set}} or \emph{universe of discourse} for the open statement. Open statements are denoted by $p(x), q(x, y)$, etc. (also called predicates).

We can \emph{quantify} an open statement with two types of quantifiers:
\begin{enumerate}
\item existential quantifier: $\exists$\\
\begin{tabular} {ll}
$\exists x$: & For some $x$\\
& For at least one $x$\\
& There exists an $x$ such that \dots
\end{tabular}
\item universal quantifier: $\forall$\\
\begin{tabular} {ll}
$\forall x$: & For all $x$\\
& For any $x$\\
& For every $x$
\end{tabular}
\end{enumerate}
Variable $x$ is called a \emph{free} variable in an open statement and a \emph{bound} variable in a quantified open statement.

Let $p(x)$ denote any open statement with a prescribed \emph{nonempty} universe. Then,
\[ \forall x \; p(x) \Rightarrow \exists x \; p(x). \]
\begin{definition} \emph{Let $p(x), q(x)$ be open statements defined for a given universe. \\\indent When the biconditional $p(a) \leftrightarrow q(a)$ is true for each replacement $a$ from the universe (that is, $p(a) \Leftrightarrow q(a)$ for each $a$ in universe), we write $\forall x \; [p(x) \Leftrightarrow q(x)]$.\\\indent If the implication $p \rightarrow q$ is true for each $a$ in the universe (that is, $p(a) \Rightarrow q(a)$ for each $a$ in universe), then we write $\forall x \; [p(x) \Rightarrow q(x)]$.}
\end{definition}
\begin{definition}\emph{For open statements $p(x), q(x)$ --- defined for a prescribed universe --- and the universally quantified statement $\forall x \; [p(x) \rightarrow q(x)]$, we define:}\end{definition}
\begin{tabular} {lclcl}
1) & Contrapositive of &$\forall x \; [p(x) \rightarrow q(x)]$& to be & $\forall x \; [\neg q(x) \rightarrow \neg p(x)]$\\
2) & Converse of &$\forall x \; [p(x) \rightarrow q(x)]$& to be & $\forall x \; [q(x) \rightarrow p(x)]$\\
3) & Inverse of &$\forall x \; [p(x) \rightarrow q(x)]$& to be & $\forall x \; [\neg p(x) \rightarrow \neg q(x)]$\\
\end{tabular}\\

For a prescribed universe and any open statements $p(x), q(x)$ in the variable $x$: 
\[\exists x \; [p(x) \wedge q(x)] \Rightarrow [\exists x \; p(x) \wedge \exists x \; q(x)]\]
\[\exists x \; [p(x) \vee q(x)] \Leftrightarrow [\exists x \; p(x) \vee \exists x \; q(x)]\]
\[\forall x \; [p(x) \wedge q(x)] \Leftrightarrow [\forall x \; p(x) \wedge \forall x \; q(x)]\]
\[\left[\forall x \; p(x) \vee \forall x \; q(x)\right] \Rightarrow \forall x \; [p(x) \vee q(x)]\]
Negation of quantifications:
\[\neg[\forall x \; p(x)] \Leftrightarrow \exists x \; \neg p(x)\]
\[\neg[\exists x \; p(x)] \Leftrightarrow \forall x \; \neg p(x)\]
\section{Sets}
\subsection{Introduction to Sets}
\begin{definition}\emph{A set is a collection of objects. The objects in a set are also called its }members, \emph{or }elements. \emph{A set is said to contain its elements.}
\end{definition}
\begin{itemize}
\item Elements of sets can be related, or completely unrelated.
\item Uppercase letters A, B, C, \dots, S, T, \dots are used to denote sets.
\item Order of elements does not matter.
\item $x \in A$ indicates that $x$ is an element of $A$.
\item $x \notin A$ indicates that $x$ is not an element of $A$.
\end{itemize}
\subsubsection{Infinite Sets}
\begin{itemize}
\item $\mathbb{N} = $ Set of natural numbers%\{1,2,3,\dots\}$
\item $\mathbb{Z} = $ Set of integers
\item $\mathbb{Q} = $ Set of rational numbers
\item $\mathbb{R} = $ Set of real numbers
\end{itemize}
\subsubsection{Set Builder Notation}
\[A = \{x|condition\}\]
The vertical line $|$ is read \emph{such that}, and the symbols $\{x| \dots \}$ are read \emph{the set of all $x$ such that ...\,}.
\begin{definition} \emph{Two sets are equal if and only if they have exactly the same elements.\footnote{$A = \{1,2,3\}$ and $B = \{1,1,2,3,2,3,3\}$ are considered to be the same set.}} \end{definition}
\subsubsection{Cardinality}
\begin{definition} \emph{Let $S$ be a set. If there are exactly $n$ distinct elements in $S$, we say $S$ is a finite set and that $n$ is the cardinality of $n$ denoted by $|S|$.} \end{definition}
\begin{definition} \emph{A set is infinite if it is not finite.} \end{definition}
\subsection{Subsets}
\begin{definition}
\emph{Set $A$ is said to be a subset of $B$ if and only if every element of $A$ is also an element of $B$. We use $A \subseteq B$ to indicate that $A$ is a subset of $B$. In predicate logic,} 
\[A \subseteq B \leftrightarrow \forall x \; [x \in A \rightarrow x \in B].\]
\end{definition}
\begin{itemize}
\item $A \subset B$ means $A$ is a proper subset of $B$ when $(A \subseteq B) \wedge (A \neq B)$
\item To show that $A=B$, we show that $(A \subseteq B) \wedge (B \subseteq A)$.
\end{itemize}
\begin{definition}\emph{The }null set, \emph{or }empty set, \emph{is the set containing no elements. It is denoted by $\emptyset$ or \{ \}.}\end{definition}
\subsubsection{The Power Set}
\begin{definition} \emph{Given a set $S$, the }power set \emph{of $S$ is the set of all subsets of $S$. We denote this by $P(S)$\footnote{In some computer science books $2^A$ is used for $P(A)$.}.}\end{definition}
For any finite set $A$ with $|A| = n \geq 0$, $A$ has $2^n$ subsets and $|P(A)| = 2^n$.
\subsection{Set Operations}
\begin{definition} \emph{For $A, B \subseteq U$, we define the following:}
\begin{itemize}
\item $A \cup B = \{x|x \in A \vee x \in B\}$
\item $A \cap B = \{x|x \in A \wedge x \in B\}$
\item $A \bigtriangleup B = \{x|x \in A \cup B \wedge x \notin A \cap B\}$
\end{itemize}
\end{definition}
\begin{definition}\emph{Let $S, T \subseteq U$. The sets $S$ and $T$ are called }disjoint, \emph{or }mutually disjoint, \emph{when $S \cap T = \emptyset$.}\end{definition}
\begin{definition}\emph{For a set $A \subseteq U$, the }complement \emph{of $A$, denoted $U-A$, or $\overline{A}$, is given by $\{x|x\in U \wedge x \notin A\}$}\end{definition}
\begin{center}
\textbf{The Laws of Set Theory}
\end{center}
\begin{tabular}  {c l l}
1) & $\overline{\overline{A}} = A$ & Law of Double Complement\\
2) & $\overline{A \cup B} = \overline{A} \cap \overline{B}$ & DeMorgan's Laws\\
& $\overline{A \cap B} = \overline{A} \cup \overline{B}$\\
3) & $A \cup B = B \cup A$ & Commutative Laws\\
& $A \cap B = B \cap A$\\
4) & $A \cup (B \cup C) = (A \cup B) \cup C$ & Associative Laws\\
& $A \cap (B \cap C) = (A \cap B) \cap C$\\
5) & $A \cup (B \cap C) = (A \cup B) \cap (A \cup C)$ & Distributive Laws\\
& $A \cap (B \cup C) = (A \cap B) \cup (A \cap C))$\\
6) & $A \cup A = A$ & Idempotent Laws\\
& $A \cap A = A$\\
7) & $A \cup \emptyset = A$ & Identity Laws\\
& $A \cap U = A$\\
8) & $A \cup \overline{A} = U$ & Inverse Laws\\
& $A \cap \overline{A} = \emptyset$\\
9) & $A \cup U = U$ & Domination Laws\\
& $A \cap \emptyset = \emptyset$\\
10) & $A \cup (A \cap B) = A$ & Absorption Laws\\
& $A \cap (A \cup B) = A$
\end{tabular}
\begin{definition}
\emph{Let $s$ be a statement dealing with the equality of two set expressions. The }dual \emph{of $s$, denoted $s^d$, is obtained from replacing (1) each occurrence of $\emptyset$ and $U$ by $U$ and $\emptyset$, respectively; and (2) each occurrence of $\cap$ and $\cup$ by $\cup$ and $\cap$, respectively.}
\end{definition}
\begin{theorem}
The Principle of Duality. \emph{Let $s$ be a theorem dealing with the equality of two set expressions. Then $s^d$, the dual of $s$, is also a theorem.}
\end{theorem}
\section{Properties of the Integers: Mathematical Induction}
\subsection{The Well-Ordering Principle}
\begin{definition}The Well-Ordering Principle. \emph{Every }nonempty \emph{subset of $\mathbb{Z}^+$ contains a smallest element. ($\mathbb{Z}^+$ is} well ordered.)
\[\forall X \subseteq \mathbb{Z}^+ \wedge X \neq \emptyset \; \exists \; a \in X \; |\; \forall x \in X: a \leq x\]
\end{definition}
We can express the set $\mathbb{Z}^+$ using the inequality symbol $\geq$:
\[\mathbb{Z}^+ = \{x \in \mathbb{Z} | x > 0\} = \{x \in \mathbb{Z} | x \geq 1\}.\]
You cannot with $\mathbb{Q}^+$ and $\mathbb{R}^+$ because they are not well ordered. If $q$ is a positive rational number, then since $0 < q/2 < q$, we would have a smaller positive rational number $q/2$.
\begin{theorem}The Principle of Mathematical Induction. \begin{em}Let $S(n)$ denote an open mathematical statement that involves one or more occurrences of the variable $n$, which represents a positive integer.
\begin{enumerate} [(a)]
\item If $S(1)$ is true; and
\item If whenever $S(k)$ is true (for some particular, but arbitrarily chosen, $k \in \mathbb{Z}^+$), then $S(k+1)$ is true;
\end{enumerate}
then $S(n)$ is true for all $n \in \mathbb{Z}^+$.\\
\emph{Proof.} Let $S(n)$ be such an open statement satisfying conditions (a) and (b), and let $F = \{t \in \mathbb{Z}^+ | S(t) \text{ is false}\}$. We wish to prove that $F=\emptyset$, so to obtain a contradiction we assume that $F\neq\emptyset$. Then by the Well-Ordering Principle, $F$ has a least element $m$. Since $S(1)$ is true, it follows that $m \neq 1$, so $m > 1$, and consequently $m - 1 \notin F$, we have $S(m-1)$ true. So by condition (b) it follows that $S((m-1)+1) = S(m)$ is true, contradicting $m \in F$. This contradiction arose from the assumption that $F \neq \emptyset$. Consequently, $F = \emptyset$.
\end{em}
\end{theorem}
\begin{theorem}The Principle of Mathematical Induction --- Alternate Form. \begin{em}Let $S(n)$ denote an open mathematical statement that involves one or more occurrences of the variable $n$, which represents a positive integer. Also let. $n_0, n_1 \in \mathbb{Z}^+$ with $n_0 \leq n_1$. 
\begin{enumerate} [(a)]
\item If $S(n_0), S(n_0 + 1), \dots , S(n_1 - 1),$ and  $S(n_1)$ are true; and
\item If whenever $S(n_0), S(n_0 + 1), \dots , S(k - 1),$ and  $S(k)$ are true for some (particular, but arbitrarily chosen), $k \in \mathbb{Z}^+$), where $k \geq n_1$, then the statement $S(k+1)$ is also true;
\end{enumerate}
then $S(n)$ is true for all $n \geq n_0$.
\end{em}
\end{theorem}
\subsection{The Division Algorithm: Prime Numbers}
\begin{definition}
\begin{em}
If $a, b \in \mathbb{Z}$ and $b \neq 0$, we say that $b$ divides $a$, and we write $b|a$, if there is an integer $n$ such that $a = bn$. When this occurs we say that $b$ is a \emph{divisor} of $a$, or $a$ is a \emph{multiple} of $b$.
\begin{itemize}
\item When $ab = 0$ for $a,b \in \mathbb{Z}$, then $a=0$ or $b=0$. We say that $\mathbb{Z}$ has no proper divisors of 0.
\item This allows us to \emph{cancel} as in $2x=2y \Rightarrow x=y$, for $x, y \in \mathbb{Z}$, because $2x=2y \Rightarrow 2(x-y)=0 \Rightarrow 2=0$ or $x=y \Rightarrow x=y$ (Note that we did not multiply both sides by $\frac{1}{2}$ which is outside the system $\mathbb{Z}$).
\item Whenever we divide by an integer $a$, we assume $a \neq 0$.
\end{itemize}
\end{em}
\end{definition}
\begin{theorem}
\begin{em}
For all $a,b,c \in \mathbb{Z}$
\begin{enumerate} [a)]
\item $1|a$ and $a|0$.
\item $[(a|b) \wedge (b|a)] \Rightarrow a=\pm b$.
\item $[(a|b) \wedge (b|c)] \Rightarrow a|c$.
\item $a|b \Rightarrow a|bx$ for all $x \in \mathbb{Z}$.
\item If $x = y + z$, for some $x, y, z \in \mathbb{Z}$, and $a$ divides two of the three integers $x, y,$ and $z$, then $a$ divides the remaining integer.
\item $[(a|b) \wedge (a|c)] \Rightarrow a|(bx+cy)$ for all $x, y \in \mathbb{Z}$.\footnote{$bx +cy$ is called a \emph{linear combination of} $b, c$.}
\item For $1 \leq i \leq n$, let $c_i \in \mathbb{Z}$. If $a$ divides each $c_i$, then $a|(c_1x_1 + c_2x_2 + \cdots + c_nx_n)$, where $x_i \in \mathbb{Z}$ for all $1 \leq i \leq n$.
\end{enumerate}
\end{em}
\end{theorem}
Using this binary operation of integer division we find ourselves in the area of mathematics called \emph{number theory}, which examines the properties of integers and other sets of numbers.

All $n \in \mathbb{Z}^+$ where $n > 1$ have at least 2 divisors, namely, 1 and $n$. Some integers, such as 2, 3, 5, 7, and 11 have exactly two divisors. These integers are called \emph{primes}. All other integers (greater than 1 and not prime) are called \emph{composite}.
\begin{lemma}
\begin{em}
If $n \in \mathbb{Z}^+$ and $n$ is composite, then there is a prime $p$ such that $p|n$.
\\
\emph{Proof.} If not, let $S$ be the set of all composite integers that have no prime divisor. If $S \neq \emptyset$, then by the Well-Ordering Principle, $S$ has a least element $m$. But if $m$ is composite, then $m = m_1m_2$, where $m_1, m_2 \in \mathbb{Z}^+$ with $1 < m_1 < m$ and $1 < m_2 < m$. Since $m = m_1m_2$, if now follows that $p|m$, and so $S = \emptyset$.
\end{em}
\end{lemma}
\begin{theorem}
\begin{em}
(Euclid) There are infinitely many primes.\\
\emph{Proof.} If not, let $p_1, p_2, \dots , p_k$ be the finite list of all primes, and let $B = p_1p_2\cdots p_k + 1$. Since $B > p_i$ for all $1 \leq i \leq k$, $B$ cannot be prime. Hence $B$ is composite. So by Lemma 4.1 there is a prime $p_j$ where $1 \leq j \leq k$ and $p_j|B$. Since $p_j|p_1p_2\cdots p_k+1$ and $p_j|p_1p_2\cdots p_k$, it follows that $p_j|1$. (Contradiction) This contradiction arises from the assumption that there are only finitely many primes; hence there are infinitely many primes.
\end{em}
\end{theorem}
\begin{theorem}
The Division Algorithm.
\begin{em}
If $a, b \in \mathbb{Z}$ with $b > 0$, then there exist unique $q, r \in \mathbb{Z}$ with $a = qb + r, 0 \leq r < b$.
\end{em}
\end{theorem}
\subsubsection{Representation of Integers in Different Bases}
Given an integer $n$ written in base 10, to write it in base $b$, we can use the division algorithm. Keep applying the algorithm until the quotient is 0:
\begin{eqnarray*}
n &=& q_0b + r_0\\
q_0 &=& q_1b + r_1\\
&\cdots&\\
q_{n-1} &=& 0 \cdot b + r_n\\
\end{eqnarray*}
Then, $n = (r_nr_{n_1}\dots r_1r_0)_b$.

One commonly used base is 16, and it is called the \emph{hexadecimal notation}. Because we only have 10 symbols in the standard base-10 system, we use the following 6 additional symbols:
\begin{center}
\begin{tabular} {llllll}
10: &A (Alfa) &11: &B (Bravo) &12: &C (Charlie) \\ 
13: &D (Delta) &14: &E (Echo) &15: &F (Foxtrot)
\end{tabular}
\end{center}

One benefit of using the hexadecimal notation is that it is easy to switch with binary. Breaking a base-2 number into blocks of four bits, then converting each block of four bits to its base-16 representation will result in the hexadecimal notation of the same number. This is because $16=2^4$, and four bits can represent a single digit in hexadecimal. We replace each hexadecimal digit into four bits to go from base-16 to base-2. 
\subsection{The Greatest Common Divisor: The Euclidean Algorithm}
\begin{definition}
\begin{em}
For $a, b \in \mathbb{Z}$, a positive integer $c$ is said to be a \emph{common divisor of a and b} if $c|a$ and $c|b$.
\end{em}
\end{definition}
\begin{definition}
\begin{em}
Let $a, b \in \mathbb{Z}$, where either $a \neq 0$ or $b \neq 0$. Then $c \in \mathbb{Z}^+$ is called a \emph{greatest common divisor} of $a, b$ if
\begin{enumerate}
\item $c|a$ and $c|b$
\item for any common divisor $d$ of $a$ and $b$, we have $d|c$.
\end{enumerate}
\end{em}
\end{definition}
\begin{theorem}
\begin{em}
For $a, b \in \mathbb{Z}^+$, there exists a unique $c \in \mathbb{Z}^+$ that is \emph{the} greatest common divisor of $a, b$.\\
\emph{Proof.} Given $a, b \in \mathbb{Z}^+$, let $S = \{as + bt | s, t \in \mathbb{Z}, as + bt > 0\}$. Since $S \neq \emptyset$, by the Well-Ordering Principle $S$ has a least element $c$. We claim that $c$ is a greatest common divisor of $a, b$.

Since $c \in S$, $c = ax + by$, for some $x, y \in \mathbb{Z}$. Consequently, if $d \in \mathbb{Z}$ and $d|a$ and $d|b$, $d|(ax+by)$, so $d|c$.

Now we show that $c|a$ and $c|b$. If $c\nmid a$, we can use the division algorithm to write $a = qc + r$, with $q, r \in \mathbb{Z}^+$ and $0 < r < c$. Then $r = a - qc = a - q(ax + by) = (1 - qx)a + (-qy)b$, so $r \in S$, contradicting the choice of $c$ as the least element of $S$. Consequently, $c|a$ and by a similar argument, $c|b$.

Hence, all $a, b \in \mathbb{Z}^+$, the greatest common divisor of $a, b$ exists, and it is unique.
\end{em}
\end{theorem}
\begin{theorem} Euclidean Algorithm.
\begin{em}
Let $a, b \in \mathbb{Z}^+$. Set $r_0 = a$ and $r_1 = b$ and apply the division algorithm $n$ times as follows:
\begin{eqnarray*}
r_0 &=& q_1r_1 + r_2\\
r_1 &=& q_2r_2 + r_3\\
&\cdots&\\
r_{n-2} &=& q_{n-1}r_{n-1} + r_n\\
r_{n-1} &=& q_nr_n
\end{eqnarray*}
Then, $r_n$, the last nonzero remainder, equals gcd$(a, b)$.\\
\emph{Proof.} To verify that $r_n =$ gcd$(a, b)$, we establish the two conditions of Definition 4.4.

If $d|a$ and $d|b$ (where $a=r_0$ and $b=r_1$), then since $r_0=q_1r_1 + r_2$, it follows that $d|r_2$. Similarly, $[(d|r_i) \wedge (d|r_{i+1})] \Rightarrow d|r_{i+2}$, so we conclude that $d|r_n$ continuing down the division process. This verifies the second condition.

To verify the first condition, we go in reverse order. From the last equation, $r_n|r_{n-1}$, so $r_n|r_{n-2}$ since $r_{n-2} = q_{n-1}r_{n-1} + r_n$. Similarly, $[(r_n|r_{i+2}) \wedge (r_n|r_{i+1})] \Rightarrow r_n|r_i$, so continuing up through the equations, we get to $r_n|r_1$ and $r_n|r_0$.

Hence, $r_n =$ gcd($a, b$).
\end{em}
\end{theorem}
\begin{theorem}
\begin{em}
If $a, b, c \in \mathbb{Z}^+$, the Diophantine equation $ax+by=c$ has an integer solution $x=x_0$, $y=y_0$ if and only if gcd($a, b) | c$.
\end{em}
\end{theorem}
\begin{theorem}
\begin{em}
For $a,b,c \in \mathbb{Z}^+$, $c$ is called a \emph{common multiple} of $a, b$ if $c$ is a multiple of both $a$ and $b$. Furthermore, $c$ is the \emph{least common multiple} of $a, b$ if it is the smallest of all positive integers that are common multiples of $a, b$. We denote $c$ by lcm($a, b$).
\end{em}
\end{theorem}
\subsection{The Fundamental Theorem of Arithmetic}
\begin{lemma}
\begin{em}
If $a, b \in \mathbb{Z}^+$ and $p$ is prime, then $p|ab \Rightarrow p|a \vee p|b$.
\end{em}
\end{lemma}
\begin{lemma}
\begin{em}
Let $a_i \in \mathbb{Z}^+$ for all $1 \leq i \leq n$. If $p$ is prime and $p|a_1a_2 \cdots a_n$, then $p|a_i$ for some $1 \leq i \leq n$.
\end{em}
\end{lemma}
\begin{theorem}
\begin{em}
Every integer $n > 1$ can be written as a product of primes uniquely, up to the order of the primes.
\end{em}
\end{theorem}
\section{Relations and Functions}
\subsection{Functions}
\begin{definition}
\begin{em}
For nonempty sets $A, B$, a \emph{function} $f$ from $A$ to $B$, denoted $f: A \rightarrow B$, is an assignment of exactly one element of $B$ to each element of $A$.
\end{em}
\end{definition}
\begin{definition}
\begin{em}
For the function $f: A \rightarrow B$, $A$ is called the \emph{domain} of $f$ and $B$ the \emph{codomain} of $f$. For each ordered pair $(a, b) \in f$, we write $f(a) = b$. $b$ is called the \emph{image} of $a$ under $f$, and $a$ is called the \emph{preimage} of $b$. The subset of $B$ consisting of all $b$ is called the \emph{range} of $f$ and is also denoted by $f(A)$ since it is the set of images under $f$.
\end{em}
\end{definition}
\begin{definition}
\begin{em}
Image of $S$ under $f$ with $f: A \rightarrow B$ and $S \subseteq A$ is $f(S) = \{f(s)| s \in S\}$.
\end{em}
\end{definition}
\subsection{Addition and Mulplication of Functions}
Let $f_1: A \rightarrow R$ and $f_2: A \rightarrow R$ be functions.\\
Then $f_1+f_2$ and $f_1f_2$ are also functions from $A \rightarrow R$ defined by:
\[(f_1 + f_2) (x) = f_1(x) + f_2 (x)\]
\[f_1f_2 (x) = f_1(x) \cdot f_2(x)\]
\subsection{One-to-one and Onto Functions}
\begin{definition}
\begin{em}
A function $f: A \rightarrow B$ is called \emph{one-to-one} or \emph{injective}, if each element of $B$ appears at most once as the image of an element $A$. That is,
\[f(x)=f(y) \Rightarrow \forall x \forall y: x = y.\]
\end{em}
\end{definition}
\begin{definition}
\begin{em}
A function is \emph{strictly increasing} if
\[\forall x \forall y ( x < y \Rightarrow f(x) < f(y)),\]
and it is \emph{strictly decreasing} if
\[\forall x \forall y ( x < y \Rightarrow f(x) > f(y)).\]
If a function is strictly increasing or strictly decreasing, it is injective.
\end{em}
\end{definition}
\begin{definition}
\begin{em}
A function $f: A \rightarrow B$ is called \emph{onto} or \emph{surjective}, if $\forall b \in B \; \exists a \in A$ with $f(a) = b$. That is, $f(A) = B$.
\end{em}
\end{definition}
\begin{definition}
\begin{em}
If a function is both injective and surjective, it is \emph{bijective}, and \emph{invertible}, meaning its inverse is also a function: \[f^{-1}: B \rightarrow A\]
\end{em}
\end{definition}
\subsection{Composition of Functions}
Let $f: A \rightarrow B$ and $g: B \rightarrow C$.\\
The composition of $f$ and $g$ is defined by \[(f \circ g) (a) = f(g(a))\]

For $f: A \rightarrow B$,
\begin{itemize}
\item $(f \circ f^{-1}) (b) = f(f^{-1}(b)) = b$
\item $(f^{-1} \circ f) (a) = f^{-1}(f(a)) = a$
\item $f^{-1} \circ f = I_A$
\item $f \circ f^{-1} = I_B$
\item If $A=B$, then $f \circ f^{-1} = f^{-1} \circ f$
\end{itemize}
\subsection{Stirling Numbers of the Second Kind}
\begin{definition}
\begin{em}
\emph{Stirling number of the second kind}, denoted by $S(m, n)$, is the number of ways to distribute $m$ distinct objects into $n$ identical containers with no container left empty, and it is given by
\[\frac{1}{n!} \sum_{k=0}^n (-1)^k \binom{n}{n-k} (n-k)^m\]
with $m \geq n$.
\end{em}
\end{definition}
For finite sets $A$ and $B$ with $|A|=m \geq n=|B|$, there are
\begin{eqnarray*}
& \displaystyle \binom{n}{n} n^m - \binom{n}{n-1} (n-1)^m + \binom{n}{n-2} (n-2)^m - \cdots + (-1)^{n-1} \binom{n}{1} 1^m\\
= & \displaystyle \sum^{n-1}_{k=0} (-1)^k \binom{n}{n-k} (n-k)^m\\
= & \displaystyle \sum^{n}_{k=0} (-1)^k \binom{n}{n-k} (n-k)^m\\
= & n! \cdot S(m, n)
\end{eqnarray*}
onto functions from $A$ to $B$.
\subsection{The Pigeonhole Principle}
\begin{definition}
\begin{em}
\emph{The Pigeonhole Principle.} If $m$ pigeons occupy $n$ pigeonholes and $m > n$, then at least one pigeonhole has two or more pigeons roosting in it.
\end{em}
\end{definition}
\subsection{Relations}
\begin{definition} \emph{Let $A$ and $B$ be sets. The cartesian product of $A$ and $B$, denoted by $A\times B$ is the set of all ordered pairs $(a, b)$ where $a \in A$ and $b \in B$.}
\[A \times B = \{ (a, b) | a \in A \wedge b \in B \}\]
\end{definition}
A relation can be defined as a subset of the cartesian product of 2 sets.
\begin{definition}
\begin{em}
A binary relation $R \subseteq A \times B$ is called a relation on $A \times B$. For any $(a, b) \in A \times B$, we write
\[a \; R \; b \text{ or } (a, b) \in R\]
if $a$ is in relation $R$ to $b$ and
\[a \; \cancel{R} \; b \text{ or } (a, b) \notin R\]
else.
\end{em}
\end{definition}
\end{document}